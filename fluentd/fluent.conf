# <source>
#   @type  forward
#   @id    input1
#   @label @mainstream
#   port  24224
# </source>

# <filter **>
#   @type stdout
# </filter>

# <label @mainstream>
#   <match docker.**>
#     @type file
#     @id   output_docker1
#     path         /fluentd/log/docker.*.log
#     symlink_path /fluentd/log/docker.log
#     append       true
#     time_key	 time
#     time_slice_format %Y%m%d
#     time_slice_wait   1m
#     time_format       %Y%m%dT%H%M%S%z
#   </match>
#   <match **>
#     @type file
#     @id   output1
#     path         /fluentd/log/data.*.log
#     symlink_path /fluentd/log/data.log
#     append       true
#     time_key	 time
#     time_slice_format %Y%m%d
#     time_slice_wait   10m
#     time_format       %Y%m%dT%H%M%S%z
#   </match>
# </label>

<source>
  @type netflow
  tag netflow.event
  port 2055
  versions [9]
</source>

# Store Data in Elasticsearch
<match netflow.*>
  @type copy

  <store>
    @type file
    path /fluentd/netflow-log
    append true
    compress gzip
  </store>

  <store>
    @type elasticsearch
    host 172.16.200.102 # This setting comes from docker-compose.yml
    port 9200
    include_tag_key true
    tag_key @log_name
    logstash_format true
    flush_interval 10s
  </store> 
</match>

<match *.**>
  @type copy

  <store>
    @type elasticsearch
    host 172.16.200.102 # This setting comes from docker-compose.yml
    port 9200
    include_tag_key true
    tag_key @log_name
    logstash_format true
    flush_interval 10s
  </store> 
</match>

## Multiple output
## match tag=td.*.* and output to Treasure Data AND file
#<match td.*.*>
#  @type copy
#  @id output_copy
#  <store>
#    @type tdlog
#    apikey API_KEY
#    auto_create_table
#    <buffer>
#      @type file
#      path /var/log/td-agent/buffer/td
#    </buffer>
#  </store>
#  <store>
#    @type file
#    path /var/log/td-agent/td-%Y-%m-%d/%H.log
#  </store>
#</match>